alg_name: sta
layers: [20]
# Use canonical SAE - automatically uses the recommended SAE for each layer
# Format: huggingface_repo_id-canonical:layer_X/width_Y/canonical
sae_paths: ['google/gemma-scope-9b-pt-res-canonical:layer_20/width_16k/canonical']
# Alternative: use width_131k canonical SAEs
# sae_paths: ['google/gemma-scope-9b-pt-res-canonical:layer_20/width_131k/canonical']
# For other layers, just change the layer number:
# sae_paths: ['google/gemma-scope-9b-pt-res-canonical:layer_24/width_16k/canonical']
trims: [0.65]
mode: act_and_freq

multiple_choice: false
save_activations: True # set to True to save the activations of the model




